<configuration>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>spiderdt</value>
  </property>

  <property>
    <name>set hive.exec.dynamic.partition</name>
    <value>true</value>
  </property>
  <property>
    <name>set hive.exec.dynamic.partition.mode</name>
    <value>nonstrict</value>
  </property>

  <property>
    <name>hive.execution.engine</name>
    <value>spark</value>
  </property>
  <property>
    <name>spark.master</name>
    <value>yarn</value>
  </property>

  <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>192.168.1.3</value>
  </property>
  <property>
      <name>spark.executor.memory</name>
      <value>2g</value>
  </property>
  <property>
      <name>spark.driver.memory</name>
      <value>1g</value>
  </property>
  <property>
      <name>hive.spark.job.monitor.timeout</name>
      <value>5m</value>
  </property>
  <property>
      <name>spark.executor.instances</name>
      <value>1</value>
  </property>
  <property>
      <name>spark.hadoop.fs.defaultFS</name>
      <value>hdfs://192.168.1.3:9000</value>
  </property>
  <property>
      <name>spark.hadoop.yarn.resourcemanager.hostname</name>
      <value>192.168.1.3</value>
  </property>
  <property>
      <name>spark.serializer</name>
      <value>org.apache.spark.serializer.KryoSerializer</value>
  </property>
  <property>
      <name>hive.optimize.metadataonly</name>
      <value>false</value>
  </property>
 <property>
      <name>hive.server2.thrift.bind.host</name>
      <value>0.0.0.0</value>
  </property>
  <property>
      <name>hive.server2.enable.doAs</name>
      <value>false</value>
  </property>
</configuration>
